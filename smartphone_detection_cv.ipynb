{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Comprehensive Computer Vision Project for Safety of Text Walkers\n",
        "\n",
        "\n",
        "####Detection of people who are looking to their smartphone\n",
        "##### Bournemouth University"
      ],
      "metadata": {
        "id": "AnXY1yIT3vIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report\n",
        "##I- Introduction\n",
        "\n",
        "The increase in daily usage of smartphones has importantly changed how humans interact with one another, conduct business, and access information, news and updates from their surroundings. We should admit that, the modern life has benefited highly from these very capable devices. But also they have brought new difficulties and potential dangers that we should consider. People's propensity to use their smartphones while walking is one of the most worrying behaviour that we face because of smartphone usage. This behaviour puts anyone nearby at danger in addition to the users themselves. Our project idea suggests a thorough computer vision based method for identifying persons who are using their smartphones while walking to solve this modern life problem. We want to increase safety, increase awareness, and finally lower the risk by implementing this system in public places, for example corner of the street near the traffic lights.\n",
        "##II- Objectives and Roadmap\n",
        "\n",
        "\n",
        "The main goal of our project is to develop nice working computer vision system that can correctly detect people using their smartphones. Our objective and motivation is to ensure that the system provides accurate predictions even in challenging scenarios such as varying lighting conditions, people who has coffee in their hand, crowded environments, varying style of walking, and varying smartphone holding styles. To achive the goals of this challenge, we plan to train and test advanced machine learning models and techniques such as Faster R-CNN, Faster R-CNN with SVM and HOG, YOLOv5 and YOLOv8.\n",
        "\n",
        "We aim to work for creating a method for identifying smartphone use while walking. The steps in our project will be as follows:\n",
        "\n",
        "1. First and foremost, our goal is to compile and analyse a sizable and varied dataset of images. This dataset will comprise people using their smartphones while walking and our dataset will also contain negative samples, for example people who are holding their smartphone but not looking to it, or people who are holding both coffee or water bottle and their smartphone.\n",
        "\n",
        "2. Then, we will search for object detection algorithms, we will choose some of them to implement our project, also we need to learn how to implement these algorithms by checking their official websites or github accounts, and analyze corresponding documentations.\n",
        "\n",
        "3. We will start training process, and we will try to improve parameters of our models, feeding more images, augmentations, trying to reduce memory usage, increasing model speed etc.\n",
        "\n",
        "4. The performance of these machine learning models, including the Faster R-CNN, Faster R-CNN with SVM and HOG, YOLOv5, and YOLOv8, will then be tested. To achieve better results we will add more images to dataset, for example when we realize that model is predicting some of the images wrongly, we will feed similar positive and negative samples similar to that image to improve model accuracy.\n",
        "\n",
        "5. Since as it is explained in project sub brief, we want our notebook to be reproducible from top to bottom, we will train our models to the fullest, with maximum number of epochs, such as the point where we see the loss of our model is getting flattened.\n",
        "\n",
        "6. Dataset and full trained models should be uploaded to online sources and imported from third party websites like roboflow, dropbox, github. So that google colab link or jupyter notebook will be accessible and anyone can produce same results like us.\n",
        "\n",
        "##III- Data Collection\n",
        "###Sources of data\n",
        "We are not using datasets from kaggle, or internet, we collected our data ourself from various sources like istock.com, pexels.com and from google images. We have collected more than a thousand images where we have people looking to their phone, or we have people who are not looking their smartphone in negative samples.\n",
        "\n",
        "###Data Annotation with Roboflow\n",
        "\n",
        "Roboflow is a easy to use platform which is designed to make annotation and data collection easier for machine learning projects. It provides simple interface for annotation and provides features for example; version control, data augmentation, and preprocessing, changing image sizes, applying various filters, removing duplicated images, marking not annotated images as negative samples etc. It is possible to load images and then share them across the teammates, so annotation can be done by team members. And also It is very easy to upload our dataset from roboflow to various platforms like google colab. We have loaded our images, annotated them and splitted our data set to three groups: Training, Testing, Validation. Their corresponding percentages are %70,%20,%10.\n",
        "\n",
        "##IV- Literature Review\n",
        "### Searching for similar projects\n",
        " \n",
        "In order to understand our way to achieve good results in detecting people who are looking to their smartphones on the street, detailed search was conducted by our team to find relevant resources,research papers, relevant code snippets articles, and projects. \n",
        "The search included various sources like academic databases, Google Scholar, as well as popular web sources, such as towardsdatascience, Medium, and GitHub, Youtube. The search terms were usually containing combinations of the spesific keywords like following: \"smartphone usage detection,\" \"pedestrian detection,\" \"object detection,\" \"computer vision,\" \"Faster R-CNN,\" \"YOLOv5,\" \"YOLOv8,\" \"HOG,\" and \"SVM.\"\n",
        "After a couple of studies have been done on detecting pedestrians and/or smartphone users, text walkers in different contexts. The following is short brief of some of the most relevant things that we found: In conclusion, the literature review make us be aware of valuable information into the current state-of-the-art in object detection projects like in our study, pedestrian and smartphone usage detection. The things that we find from these searches informed us about deciding of algorithms and techniques that we will choose to use in our project.\n",
        "\n",
        "### Documentations\n",
        "\n",
        "After careful literature research we decided to implement faster cnn and yolo (you only look once) library as early we explained results were pretty well. At that point, analyzing and reading the official documentations were very helpful because there was a guideline to train our custom datasets and detailed explanation about other details. For example, trying our model on a video. At the end of the notebook we examine our yolov8 model on a video and we are truly satisfied with the results that we achived there.\n",
        "\n",
        "##V- Object Detection Algorithms\n",
        "\n",
        "### Faster R-CNN\n",
        "\n",
        "Recently, the Faster R-CNN showed impressive results on a number of object detection benchmarks. On two frequently used face detection benchmarks, it is reported state-of-the-art results using a Faster R-CNN model trained on large scale datasets. In many computer vision tasks, deep convolutional neural networks (CNNs) have taken control. Region-based CNN detection approaches are currently the dominant paradigm in object detection. Three generations of region-based CNN detection models have been suggested in recent years, each with greater performance and quicker processing speed, because the field is expanding so quickly.(Jiang, 2017)\n",
        "\n",
        "Faster R-CNN is one of the state-of-the-art object detection algorithm that contains Region Proposal Network (RPN) with a Fast R-CNN model. The RPN is generating example object bounding boxes, which are passed to Fast R-CNN model for classification and determining bounding boxes . This two stage approach is enabling Faster R-CNN to achieve good accuracy and also it is maintaining reasonable time and speed.\n",
        " \n",
        "#### Integration to the project\n",
        "\n",
        "Faster R-CNN was implemented as one of our primary algorithm for object detection, for detecting people using smartphones on the street while walking or standing. The model was trained on the annotated dataset that we have, focusing on identifying pedestrians and their smartphone usage. We have trained the model for 10 epochs. And It takes around two hours to complete it in our local computer, time is subject to change if one is using another local machine depending on the hardware available.\n",
        "\n",
        "\n",
        "### Support Vector Machines (SVM) and Histogram of Oriented Gradients (HOG) on top of Faster R-CNN\n",
        "\n",
        "SVM is supervised machine learning algorithm that we mostly use  for classification tasks in daily applications or in classes in university, we are familiar with that algorithm. It finds the optimal hyperplane which is separating the classes in space best. HOG is feature descriptor that also know from our computer vision lectures, we have used it in lecture and lab.It captures the distribution of gradients and It may capture edge directions in an image, and than make it suitable for object recognition tasks.\n",
        "SVM and hog is used like a secondary filter on top of the fastercnn algorithm that we train, we aim to remove false positives that fastercnn predict by the help of svma and hog, by doing so we will be able to combine traditional computer vision approaches with the advanced models like neural networks. So this will become a hybrid solution to our problem.\n",
        "\n",
        "#### Integration to the project\n",
        "\n",
        "The SVM and HOG features were combined with Faster R-CNN to improve the detection of smartphone users. HOG features are extracted from the potential pedestrian or people with phone regions proposed by the RPN, and then SVM classifier is trained on that regions to identify people with smartphone on these features.\n",
        "\n",
        "### YOLOv5\n",
        "YOLO is a novel method of object detection.Classifiers from earlier work on object detection are repurposed for shape detection. Rather, it conceptualise object detection as a regression issue to bounding boxes with spatial separations and related class probabilities. Bounding boxes and class probabilities are directly predicted by a single neural network from entire images in a single assessment. Since the entire detection pipeline consists of a single network, detection performance can be optimised from beginning to end.(Redmon, 2016)\n",
        "\n",
        "YOLOv5 (You Only Look Once) is free and easy to use efficient, single stage object detection algorithm. It divides input image into grid and then it is predicting bounding boxes and class probabilities. YOLOv5 is known for its performance, how easy of its implementation, and very good accuracy in various object detection tasks.\n",
        "\n",
        "#### Integration to the project\n",
        "\n",
        "YOLOv5 is implemented as alternative object detection algorithm for detecting people with smartphone on the street. Our model is trained on the annotated dataset that we upload from roboflow. And we are able to compare its performance with Faster R-CNN and the combined SVM-HOG approach.\n",
        "\n",
        "### YOLOv8\n",
        "In a wide range of applications, including those in the domains of autonomous cars, robotics, video surveillance, and augmented reality, real-time object identification has become an essential element. The YOLO (You Only Look Once) framework has distinguished itself among the many object detection algorithms for its exceptional balance of speed and precision, enabling the quick and accurate identification of objects in photos. The YOLO family has gone through several incarnations since its creation, each improving upon the ones before it to address flaws and improve performance.(Aboah, 2023)\n",
        "\n",
        "YOLOv8 is more recent version of You only look once, YOLO family of object detection algorithms. It includes various improvements, for example it has new backbone architecture, new modified loss function, and improved data augmentation techniques. These improvements make it possible for YOLOv8 to achieve higher accuracy and faster inference speed compared to its predecessors.\n",
        "\n",
        "#### Integration to the project\n",
        "\n",
        "YOLOv8 was also implemented as an object detection algorithm for detecting people using smartphones on the street. The model was trained on the annotated dataset and its performance was compared with Faster R-CNN, the combined SVM-HOG approach, and YOLOv5 to evaluate the most effective method for the task.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##VI- Implementation and Evaluation\n",
        "### Training the models\n",
        "\n",
        "The one that we have selected as object detection algorithms, they are including Faster R-CNN, the hybrid SVM-HOG approach, YOLOv5, and YOLOv8, were trained on our annotated dataset over roboflow. The dataset was split into training, validation, and testing sets. Hyperparameters were tuned to optimize the models' performance, and we have trained fastercnn for 10 epochs around two hours, yolov5 for 150 epochs around 2.5 hours, yolov8 for 150 epochs around 2.3 hours.\n",
        " \n",
        "### Comparing the results of different algorithms\n",
        "\n",
        "The performance of different algorithms that we used and trained was compared based on specific evaluation metrics. The aim of comparison is to identify the most effective method for our project which is detecting people looking to their smartphone on the street. Results were analyzed to determine both strengths and weaknesses of each of the approaches, as well as to understand the reasons that contributed to their performance.\n",
        "\n",
        "### Discussion of challenges and limitations\n",
        "\n",
        "During implementation and evaluation, we have encountered with several challenges and limitations. These includes the varying quality of images that we feed as input to the algorithms, the diversity of people poses and diversity of smartphones, and patterns that people hold their phone or other things at hand like wallet, coffe, watch or laptop etc. And the computational resources required for training and inference, because It takes too much time for us to make our notebook work from bottom to top it takes around 9 hours, also we had difficulties about CUDA memorya and colab usage, since we are using free version in our local computers, training process stopped for several times because of that reasons.\n",
        "\n",
        "##VII- Conclusion\n",
        "\n",
        "### Summary of findings\n",
        "\n",
        "Our project is aiming to detect people who are looking to their smartphones on the street using different object detection algorithms. The implemented models were Faster R-CNN, the combined SVM-HOG approach, YOLOv5, and YOLOv8. Based on the performance metrics, we have couple of very effective methods that we have trained. It offers insights into the factors that contributed to models success. Additionally, the challenges and limitations faced during the project were discussed.\n",
        "\n",
        "### Implications and potential applications\n",
        "\n",
        "In light of this, there may be applications.\n",
        "There are numerous implications and potential applications for the study's findings. The developed system might be used for research on peoples behaviour, community safety precaustions, or urban planning. Additionally, it could be used to enhance people identification and boost the safety of autonomous vehicle systems' intelligent transportation systems.\n",
        "\n",
        "### Future work and recommendations\n",
        "\n",
        "The focus of future study may be on enhancing the models by including more data or looking into more sophisticated techniques and/or approaches. For example using attention mechanisms or adding temporal data from video sequences. Additional search might look into ethical implications because of people's faces and any biases in the detection system to make sure trained model's solution is fair, transparent, and with out privacy concerns.\n",
        "\n",
        "\n",
        "## Important Notes About Notebook\n",
        "\n",
        "To achieve reproducibility, and prevent CUDA memory crash while running the notebook, we will train all models for just one epoch. and also to reduce memory we added a 'number of images' variable for dataset loader function. Since our dataset contains more than a thousand images, we wanted to make number of images as input variable to limit usage of memory.\n",
        "\n",
        "To prevent deterministic behavior error of pytorch we will edit cublas workspace config at the beginning.\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "\n",
        "In order to make the notebook partially producible as well for different machine learning models we will use 3 different dataset format, coco for fastercnn, and roboflow template for yolov5 and yolov8.\n",
        "\n",
        "We will import and upload fully trained models from dropbox, since we use free version, links are valid for 1 month, they can be updated and their time can be extended if we buy subscription.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        " \n",
        "\n",
        "## References, Resource, and Links\n",
        "\n",
        "Jiang, H. and Learned-Miller, E., 2017, May. Face detection with the faster R-CNN. In 2017 12th IEEE international conference on automatic face & gesture recognition (FG 2017) (pp. 650-657). IEEE.\n",
        "\n",
        "Redmon, J., Divvala, S., Girshick, R. and Farhadi, A., 2016. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 779-788).\n",
        "\n",
        "Aboah, A., Wang, B., Bagci, U. and Adu-Gyamfi, Y., 2023. Real-time multi-class helmet violation detection using few-shot data sampling technique and yolov8. arXiv preprint arXiv:2304.08256.\n",
        "\n",
        "### Code Cells\n",
        "2.1 The Dataset\n",
        "\n",
        "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
        "\n",
        "https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
        "\n",
        "2.3 Augmentations\n",
        "\n",
        "https://albumentations.ai/docs/api_reference/pytorch/transforms/#albumentations.pytorch.transforms.ToTensorV2\n",
        "\n",
        "https://github.com/albumentations-team/albumentations\n",
        "\n",
        "2.4 DataLoaders\n",
        "\n",
        "https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "2.5 Pretained model and 2.6 Training\n",
        "\n",
        "https://pytorch.org/vision/stable/models.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n",
        "\n",
        "https://pytorch.org/vision/stable/_modules/torchvision/models/detection/faster_rcnn.html#FastRCNNPredictor\n",
        "\n",
        "2.7 Filtering the outputs\n",
        "\n",
        "https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c\n",
        "\n",
        "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#object-detection-finishing-touches-nms\n",
        "\n",
        "https://pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/\n",
        "\n",
        "4 Yolov5\n",
        "\n",
        "https://docs.ultralytics.com/yolov5/\n",
        "\n",
        "https://github.com/ultralytics/yolov5\n",
        "\n",
        "5 Yolov8\n",
        "\n",
        "https://docs.ultralytics.com/\n",
        "\n",
        "https://github.com/ultralytics/ultralytics\n",
        "\n",
        "### Example Projects:\n",
        "\n",
        "https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\n",
        "\n",
        "https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\n",
        "\n",
        "\n",
        "###Helpful resources:\n",
        "\n",
        "https://www.youtube.com/watch?v=GRtgLlwxpc4&t=536s&pp=ygUmY3VzdG9tIGRhdGFzZXQgb2JqZWN0IGRldGVjdGlvbiB5b2xvdjU%3D\n",
        "\n",
        "https://www.youtube.com/watch?v=fu2tfOV9vbY\n",
        "\n",
        "https://www.youtube.com/watch?v=fhzCwJkDONE&t=845s\n",
        "\n",
        "https://www.youtube.com/watch?v=PPpKlPYL95c&t=868s\n",
        "\n",
        "###Sources for Data Collection\n",
        "\n",
        "https://www.pexels.com/search/people%20talking%20on%20the%20smartphone%20on%20foot/\n",
        "\n",
        "https://www.istockphoto.com/search/2/image-film?phrase=people%20walking%20with%20smartphone\n",
        "\n",
        "https://www.istockphoto.com/search/2/image-film?family=creative&phrase=people%20walking%20in%20street%20and%20smartphone\n",
        "\n",
        "https://www.google.com/search?q=people+walking+while+looking+to+phone&rlz=1C1SQJL_enTR910TR910&sxsrf=APwXEdeOojQij3NQK136Ywc-yg_nv00eqQ:1683239646928&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjKk-ip3Nz-AhWGWcAKHUi4Bq8Q_AUoAXoECAEQAw&biw=1474&bih=762&dpr=1.25\n",
        "\n",
        "<br>\n",
        "<br>\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "6QSY9gvw0LMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation"
      ],
      "metadata": {
        "id": "yK2LS1sm2UU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"color:white;\n",
        "           display:fill;\n",
        "           border-radius:5px;\n",
        "           background-color:#9041A0;\n",
        "           font-size:110%;\n",
        "           font-family:Segoe UI;\n",
        "           letter-spacing:0.5px\">\n",
        "\n",
        "<p style=\"padding: 10px;\n",
        "              color:white;\">\n",
        "    Outline of Implementation:\n",
        "    <li style=\"padding-left:1em\">1.   Importing Libraries</li>\n",
        "    <li style=\"padding-left:1em\">2.   FasterCNN</li>\n",
        "    <li style=\"padding-left:1em\">2.1  The Dataset</li>\n",
        "    <li style=\"padding-left:1em\">2.2  Visualization</li>\n",
        "    <li style=\"padding-left:1em\">2.3  Augmentations</li>\n",
        "    <li style=\"padding-left:1em\">2.4  Data Loaders</li>\n",
        "    <li style=\"padding-left:1em\">2.5  Pre-trained Model</li>\n",
        "    <li style=\"padding-left:1em\">2.6 Training</li>\n",
        "    <li style=\"padding-left:1em\">2.7  Filtering The Outputs</li>\n",
        "    <li style=\"padding-left:1em\">2.8  Testing The Model</li>\n",
        "    <li style=\"padding-left:1em\">3    Hybrids of Traditional Approach and Neural Networks</li>\n",
        "    <li style=\"padding-left:1em\">3.1. SVM and HOG</li>\n",
        "    <li style=\"padding-left:1em\">4.   Yolov5</li>\n",
        "    <li style=\"padding-left:1em\">4.1  Training</li>\n",
        "    <li style=\"padding-left:1em\">4.2  Testing</li>\n",
        "    <li style=\"padding-left:1em\">4.3  Interactive Visualization</li>\n",
        "    <li style=\"padding-left:1em\">5.   Yolov8</li>\n",
        "    <li style=\"padding-left:1em\">5.1  Training</li>\n",
        "    <li style=\"padding-left:1em\">5.2  Testing</li>\n",
        "    <li style=\"padding-left:1em\">5.3  Testing Model On Video</li>\n",
        "    <li style=\"padding-left:1em\">6.   Importing Fully Trained Models</li>\n",
        "    <li style=\"padding-left:1em\">6.1  Loading FasterCNN</li>\n",
        "    <li style=\"padding-left:1em\">6.2  Defining Display Functions</li>\n",
        "    <li style=\"padding-left:1em\">6.3  Loading SVM</li>\n",
        "    <li style=\"padding-left:1em\">6.4  Comparing CNN and Hybrid Approach</li>\n",
        "    <li style=\"padding-left:1em\">6.5  Loading Yolov5</li>\n",
        "    <li style=\"padding-left:1em\">6.6  Loading Yolov8</li>\n",
        "    <li style=\"padding-left:1em\">6.7  Testing Yolov8 Model On Video</li>\n",
        "             </p> </div>"
      ],
      "metadata": {
        "id": "Y9mPuEGEtS44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are importing some necessary files that we upload to github, for example image for introduction and video to test model.\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "!git clone https://github.com/musacim/ObjectDetection_files.git >/dev/null\n",
        "!mv /content/ObjectDetection_files/* /content/"
      ],
      "metadata": {
        "id": "EKPX2XQvE_aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image1 = mpimg.imread('/content/crossing_road_smartphone.png')\n",
        "image2 = mpimg.imread('/content/detection_system.png')\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(14, 14))\n",
        "axs[0].imshow(image2)\n",
        "axs[0].set_title('With Detection System')\n",
        "axs[0].axis('off')\n",
        "axs[0].text(0.5, -0.1, '*Our fully trained YOLOv8 model is used for detection',\n",
        "            size=8, ha=\"center\", transform=axs[0].transAxes)\n",
        "axs[1].imshow(image1)\n",
        "axs[1].set_title('Without Detection System')\n",
        "axs[1].axis('off')\n",
        "\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Fw0hYmhVLHCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Positive and Negative Image Samples\n",
        "### Dataset\n",
        "We have 1010 images in our dataset, 70% of these images are positive samples and 30% is negative samples. All of the images are collected and annotated by project team. We use roboflow for;storing images,annotating, splitting images for test,train, and validation folders and also importing them into colab.\n",
        "\n",
        "### Positive and Negative Samples\n",
        "\n",
        "Our aim is to detect people who are looking their phone, they might walk or stand but for us to label them as positive and annotate the object, there should be a single person or multiple people who are looking to their phone, if they look somewhere else but holding their phone it is negative class, if they are looking to their book, computer or coffe bottle these are negative samples. Talking to phone or showing their phone to their friends are negative samples as well."
      ],
      "metadata": {
        "id": "60UB7A2x1qOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 4, figsize=(10, 6))\n",
        "\n",
        "axs[0, 0].imshow(Image.open(\"/content/ps0.jpg\"))\n",
        "axs[0, 1].imshow(Image.open(\"/content/ps1.jpg\"))\n",
        "axs[0, 2].imshow(Image.open(\"/content/ps2.jpg\"))\n",
        "axs[0, 3].imshow(Image.open(\"/content/ps3.jpg\"))\n",
        "axs[1, 0].imshow(Image.open(\"/content/ns0.jpg\"))\n",
        "axs[1, 1].imshow(Image.open(\"/content/ns1.jpg\"))\n",
        "axs[1, 2].imshow(Image.open(\"/content/ns2.jpg\"))\n",
        "axs[1, 3].imshow(Image.open(\"/content/ns3.jpg\"))\n",
        "\n",
        "axs[0, 0].set_title(\"Positive Samples\")\n",
        "axs[1, 0].set_title(\"Negative Samples\")\n",
        "for ax in axs.flat:\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "fig.subplots_adjust(hspace=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Be1Rhqp3zxcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Importing Libraries"
      ],
      "metadata": {
        "id": "KTGVKSFFFXIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch --quiet\n",
        "!pip install torchvision --quiet\n",
        "\n",
        "# Install dependencies\n",
        "!pip install albumentations\n",
        "!pip install pycocotools --quiet\n",
        " \n",
        "\n",
        "#Yolov5, its requirements and roboflow\n",
        "!git clone https://github.com/ultralytics/yolov5 --quiet\n",
        "!pip install -r requirements.txt --quiet   \n",
        "!pip install roboflow --quiet\n",
        "\n",
        "#Yolov8\n",
        "!pip install ultralytics --quiet\n",
        "\n",
        "# Clone TorchVision repo and copy helper files\n",
        "!git clone https://github.com/pytorch/vision.git\n",
        "%cd vision\n",
        "!git checkout v0.3.0\n",
        "%cd ..\n",
        "!cp vision/references/detection/utils.py ./\n",
        "!cp vision/references/detection/transforms.py ./\n",
        "!cp vision/references/detection/coco_utils.py ./"
      ],
      "metadata": {
        "id": "87fUFqOBp3mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basic python and ML Libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# for ignoring warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# We will be reading images using OpenCV\n",
        "import cv2\n",
        "\n",
        "# matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "\n",
        "# torch and torchvision libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as torchtrans  \n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import torch.utils.data as utils_data\n",
        "\n",
        "# helper libraries\n",
        "from custom_engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "# for image augmentations and tools\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "import pycocotools\n",
        "\n",
        "# sklearn functions and models\n",
        "from joblib import load\n",
        "from skimage.feature import hog\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        " \n",
        "#Yolo and roboflow\n",
        "from ultralytics import YOLO\n",
        "from roboflow import Roboflow \n",
        "\n",
        "#some libraries for visualization for videos\n",
        "from moviepy.editor import *\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n"
      ],
      "metadata": {
        "id": "xUuuAH2Up-4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pytorch gives error about deterministic behavior, providing cublas_workspace_config solves it.\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  "
      ],
      "metadata": {
        "id": "T0h_ur8JdVsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Faster CNN"
      ],
      "metadata": {
        "id": "1P3z_mVgn52f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkwGcwwlh_k0"
      },
      "source": [
        "##2.1 The Dataset "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We are uploading the dataset here from roboflow to use it with fastercnn. \n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"7QnIUGHlluFrILkFBubf\")\n",
        "project = rf.workspace(\"adasd-ukmfp\").project(\"people_with_smartphone\")\n",
        "dataset = project.version(4).download(\"coco\")\n"
      ],
      "metadata": {
        "id": "DftbfvIm-cq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are taking annotations into dataframe, dataframe consists of x value, y value, width, height, filename, and has_object value \n",
        "#which indicates that image has object in it.\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "with open('/content/people_with_smartphone-4/train/_annotations.coco.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "df1=pd.DataFrame(data['images'])\n",
        "df2=pd.DataFrame(data['annotations'])\n",
        "df_annotations=df2[['id','image_id','bbox']]\n",
        "df_images=df1[['id','file_name']]\n",
        "merged_df = df_images.merge(df_annotations, left_on='id', right_on='image_id', how='left')\n",
        "merged_df = merged_df.drop(columns=['image_id','id_y','id_x'])   \n",
        "merged_df.rename(columns={'bbox': 'bbox_values'}, inplace=True)   \n",
        "merged_df['bbox_values'].fillna(value=0, inplace=True)\n",
        "merged_df['has_object'] = merged_df['bbox_values'].apply(lambda x: 0 if x == 0 or x == [0, 0, 0, 0] else 1)\n",
        "def process_bbox_values(row):\n",
        "    if row['has_object'] == 1:\n",
        "        x, y, w, h = row['bbox_values']\n",
        "        width = w\n",
        "        height = h\n",
        "    else:\n",
        "        x, y, width, height = 0, 0, 0, 0\n",
        "\n",
        "    return pd.Series([x, y, width, height])\n",
        "\n",
        "merged_df[['x', 'y', 'width', 'height']] = merged_df.apply(process_bbox_values, axis=1)\n",
        "merged_df.rename(columns={'file_name': 'filename'}, inplace=True)\n",
        "merged_df=merged_df.drop(columns=['bbox_values'])\n",
        "merged_df=merged_df[['x','y','width','height','filename','has_object']]\n",
        "df=merged_df"
      ],
      "metadata": {
        "id": "ApjJbjAjFHgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "DYAZKO_WRA6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "7K9qfPoTh_k1"
      },
      "source": [
        "# We are defining training directory and testing directory.\n",
        "train_images_dir = '/content/people_with_smartphone-4/train'\n",
        "test_images_dir = '/content/people_with_smartphone-4/test/'\n",
        "\n",
        "#We are defining custom dataset for object detection\n",
        "#We are loading the images from a directory and their corresponding annotations\n",
        "#There are also transform and display function\n",
        "\n",
        "class PersonDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, files_dir, df,number_of_images_to_load, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.files_dir = files_dir\n",
        "        self.df = df\n",
        "        self.number_of_images_to_load=number_of_images_to_load\n",
        "         \n",
        "        self.imgs = sorted([image for image in os.listdir(files_dir) if not image.endswith('.json')])[:number_of_images_to_load]\n",
        "        \n",
        "        self.classes = [_, 'person_lookingto_phone']\n",
        "         \n",
        "    def __getitem__(self, idx):\n",
        "      img_name = self.imgs[idx]\n",
        "      image_path = os.path.join(self.files_dir, img_name)\n",
        "\n",
        "      img = cv2.imread(image_path)\n",
        "      img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "      img_res = img_rgb / 255.0\n",
        "\n",
        "      \n",
        "      row = self.df[self.df['filename'] == img_name]\n",
        "\n",
        "      if row.empty:\n",
        "          has_object = 0\n",
        "      else:\n",
        "          has_object = row['has_object'].values[0]\n",
        "\n",
        "      if has_object:\n",
        "          xmin = row['x'].values[0]\n",
        "          xmax = xmin + row['width'].values[0]\n",
        "          ymin = row['y'].values[0]\n",
        "          ymax = ymin + row['height'].values[0]\n",
        "\n",
        "          boxes = [[xmin, ymin, xmax, ymax]]\n",
        "          boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "          labels = [1]\n",
        "          area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "          iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "      else:\n",
        "          boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "          labels = []\n",
        "          area = torch.zeros((0,), dtype=torch.float32)\n",
        "          iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "      labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "      target = {\n",
        "          \"boxes\": boxes,\n",
        "          \"labels\": labels,\n",
        "          \"area\": area,\n",
        "          \"iscrowd\": iscrowd,\n",
        "          \"image_id\": torch.tensor([idx]),\n",
        "      }\n",
        "\n",
        "      if self.transforms:\n",
        "          sample = self.transforms(image=img_res, bboxes=target['boxes'], labels=labels)\n",
        "          img_res = sample['image']\n",
        "          target['boxes'] = torch.Tensor(sample['bboxes']) if len(sample['bboxes']) > 0 else torch.zeros((0, 4), dtype=torch.float32)\n",
        "\n",
        "      return img_res, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def show_image_with_bbox(self, idx):\n",
        "        img, target = self.__getitem__(idx)\n",
        "        img = img * 255.0  # Undo normalization\n",
        "        fig, ax = plt.subplots(1)\n",
        "        ax.imshow(img.astype(np.uint8))\n",
        "\n",
        "        for box in target['boxes']:\n",
        "            xmin, ymin, xmax, ymax = box\n",
        "            width = xmax - xmin\n",
        "            height = ymax - ymin\n",
        "            rect = patches.Rectangle((xmin, ymin), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gktXqEih_k3"
      },
      "source": [
        "##2.2 Visualization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To be sure about boxes around our object, we display an example.\n",
        "dataset = PersonDataset(train_images_dir, df,50)\n",
        "dataset.show_image_with_bbox(11) \n"
      ],
      "metadata": {
        "id": "l1AHysuHUO_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eDYqOTnh_k4"
      },
      "source": [
        "##2.3 Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nEy7h5mNh_k5"
      },
      "source": [
        "#We are applying augmentations to images by ensuring that the boxes that we draw are still aligned with the object.\n",
        "def get_transform(train):\n",
        "  if train:\n",
        "    return A.Compose(\n",
        "      [\n",
        "        A.HorizontalFlip(0.5),\n",
        "         \n",
        "        ToTensorV2(p=1.0) \n",
        "      ],\n",
        "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )\n",
        "  else:\n",
        "    return A.Compose(\n",
        "      [ToTensorV2(p=1.0)],\n",
        "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmAdf8Yih_k5"
      },
      "source": [
        "##2.4 Dataloaders\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gxoYjbRUh_k5"
      },
      "source": [
        "#We are creating dataloader objects for training and testing, in order to ensure that CUDA memory is not full,\n",
        "#for training the model we are loading just 50 images, at the end of the notebook. We load the full trained models where we use \n",
        "#all the images which are more than a thousand and we train models with 150 epochs.\n",
        "\n",
        "number_of_images_to_load_train=50\n",
        "number_of_images_to_load_test=250\n",
        " \n",
        "dataset = PersonDataset(train_images_dir,df,number_of_images_to_load_train, transforms=get_transform(train=True))\n",
        "dataset_test = PersonDataset(test_images_dir,df,number_of_images_to_load_test, transforms=get_transform(train=False))\n",
        "\n",
        " \n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "    images = list(images)\n",
        "    targets = list(targets)\n",
        "\n",
        "    return images, targets\n",
        "\n",
        " \n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "  dataset,\n",
        "  batch_size=2 ,\n",
        "  shuffle=True,\n",
        "  num_workers=2,\n",
        "  collate_fn=collate_fn,\n",
        "\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "  dataset_test,\n",
        "  batch_size=2,\n",
        "  shuffle=False,\n",
        "  num_workers=2,\n",
        "  collate_fn=collate_fn,\n",
        "\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HpLH-obh_k4"
      },
      "source": [
        "##2.5 Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PLGLZVbRh_k4"
      },
      "source": [
        "#We are loading pretrained model for fastercnn so that we won't start from scratch\n",
        "\n",
        "def get_object_detection_model(num_classes):\n",
        "   \n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPsWQFoah_k5"
      },
      "source": [
        "##2.6 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qP7T0cA-h_k5"
      },
      "source": [
        "#We are preparing for training, defining device as gpu if available,\n",
        "#we have 2 number of classes one for object itself and one more background.\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "num_classes = 2 \n",
        "model = get_object_detection_model(num_classes)\n",
        "model.to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "  optimizer,\n",
        "  step_size=3,\n",
        "  gamma=0.1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "75qmxd-yh_k5"
      },
      "source": [
        "#We are training the model here, for full training that we use at the end of notebook we trained\n",
        "#the model for 10 epochs, in order to ensure that CUDA memory is not full we train it just for one epoch here,\n",
        "#because notebook should be working from top to bottom without intervention\n",
        "\n",
        "num_epochs=1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    lr_scheduler.step()\n",
        "    evaluate(model, data_loader_test, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmPxIhhHh_k5"
      },
      "source": [
        "##2.7 Filtering the outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5fsSRjrIh_k5"
      },
      "source": [
        "# We are filtering outputs with non maximum suppression technique, this technique is used to remove\n",
        "# unnecessary redundant bounding boxes, because model might predict several bounding boxes at the same location,\n",
        "# we want to filter them by nms and we use certain threshold\n",
        "\n",
        "def apply_nms(orig_prediction, iou_thresh=0.3, score_thresh=0.7):\n",
        "    \n",
        "    mask = orig_prediction['scores'].cpu() > score_thresh\n",
        "    filtered_boxes = orig_prediction['boxes'].cpu()[mask]\n",
        "    filtered_scores = orig_prediction['scores'].cpu()[mask]\n",
        "    filtered_labels = orig_prediction['labels'].cpu()[mask]\n",
        "\n",
        "    keep = torchvision.ops.nms(filtered_boxes, filtered_scores, iou_thresh)\n",
        "\n",
        "    final_prediction = orig_prediction\n",
        "    final_prediction['boxes'] = filtered_boxes[keep]\n",
        "    final_prediction['scores'] = filtered_scores[keep]\n",
        "    final_prediction['labels'] = filtered_labels[keep]\n",
        "\n",
        "    return final_prediction\n",
        "\n",
        "def torch_to_pil(img):\n",
        "  return torchtrans.ToPILImage()(img).convert('RGB')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjAbCCp8h_k6"
      },
      "source": [
        "##2.8 Testing The Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #We are defining two functions here, one for drawing bounding boxes into image\n",
        " #and another function is used to display the image with that bounding box.\n",
        "\n",
        "def plot_img_bbox(ax, img, target):\n",
        "    ax.imshow(img)\n",
        "\n",
        "    for box, score in zip(target['boxes'], target['scores']):\n",
        "        x, y, width, height = box[0], box[1], box[2] - box[0], box[3] - box[1]\n",
        "\n",
        "        rect = patches.Rectangle(\n",
        "            (x, y),\n",
        "            width, height,\n",
        "            linewidth=2,\n",
        "            edgecolor='r',\n",
        "            facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x, y, f\"{score:.2f}\", fontsize=12, bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "\n",
        " \n",
        "def display_images(test_dataset, model, num_images, columns=2):\n",
        "    rows = int(np.ceil(num_images / columns))\n",
        "    fig, axes = plt.subplots(rows, columns, figsize=(3 * columns, 3 * rows))\n",
        "\n",
        "     \n",
        "    axes = axes.ravel()\n",
        "    for ax in axes[num_images:]:\n",
        "        fig.delaxes(ax)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    sequential_indices = list(range(num_images))\n",
        "\n",
        "    for i, sequential_index in enumerate(sequential_indices):\n",
        "        img, _ = test_dataset[sequential_index]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = model([img.to(device)])[0]\n",
        "\n",
        "        nms_prediction = apply_nms(prediction, iou_thresh=0.01, score_thresh=0.01)\n",
        "        plot_img_bbox(axes[i], torch_to_pil(img), nms_prediction)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pEwtwelTs1sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_images(dataset_test, model, num_images=4, columns=4)"
      ],
      "metadata": {
        "id": "fovM0Mlw2jRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Hybrids of Traditional Approach and Neural Networks"
      ],
      "metadata": {
        "id": "aAiVWFrqvDjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We have trained our fastercnn model but sometimes, false positives can sometimes occur.\n",
        "#We will train svm by using HOG as a feature extraction method and \n",
        "#SVM and hog will be our secondary filter to remove false positives.\n",
        "#The idea is that HOG features will be extracted from locations from the image that FasterCNN model determine\n",
        "#then we will feed these features to svm classifier to distinguish between true and false positives."
      ],
      "metadata": {
        "id": "yCGJe_kvts8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 SVM and HOG"
      ],
      "metadata": {
        "id": "dbTEA4dKcaFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are defining our hog feature funtions\n",
        "def extract_hog_features(img, size=(64, 128)):\n",
        "    img_res = cv2.resize(img, size, cv2.INTER_AREA)\n",
        "    features, _ = hog(img_res, orientations=9, pixels_per_cell=(8, 8),\n",
        "                      cells_per_block=(2, 2), visualize=True, multichannel=True)\n",
        "    return np.array(features)\n"
      ],
      "metadata": {
        "id": "7BXdvGDyO4M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hog_features_from_bbox(img, bbox):\n",
        "    x1, y1, x2, y2 = map(int, bbox)\n",
        "    cropped_img = img[y1:y2, x1:x2]\n",
        "    img_res = cv2.resize(cropped_img, (64, 128), cv2.INTER_AREA)\n",
        "    img_res /= 255.0\n",
        "    features = extract_hog_features(img_res)\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "vop8GGeYO4Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the data to train svm classifier\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    img_name = row['filename']\n",
        "    image_path = os.path.join(train_images_dir, img_name)\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "\n",
        "    has_object = row[\"has_object\"]\n",
        "    if has_object:\n",
        "        bbox = [row['x'], row['y'], row['x'] + row['width'], row['y'] + row['height']]\n",
        "        features = extract_hog_features_from_bbox(img_rgb, bbox)\n",
        "        X.append(features)\n",
        "        y.append(1)\n",
        "    else:\n",
        "        X.append(extract_hog_features(img_rgb))\n",
        "        y.append(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "\n",
        "clf = SVC(kernel='linear', C=1, probability=True)\n",
        "clf.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "BG25pkTMO4Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the function that we will use svm on images if it has a false positive\n",
        "def apply_svm_on_proposals(model, img, threshold=0.5):\n",
        "    \n",
        "    img_tensor = torch.tensor(img / 255, dtype=torch.float).permute(2, 0, 1).to(device)\n",
        "    img_tensor = img_tensor.unsqueeze(0)\n",
        "\n",
        "     \n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_tensor)\n",
        "\n",
        "     \n",
        "    scores = outputs[0]['scores']\n",
        "    boxes = outputs[0]['boxes']\n",
        "    labels = outputs[0]['labels']\n",
        "\n",
        "    filtered_indices = [idx for idx, score in enumerate(scores) if score > threshold]\n",
        "    filtered_boxes = boxes[filtered_indices]\n",
        "    filtered_labels = labels[filtered_indices]\n",
        "\n",
        "     \n",
        "    final_boxes = []\n",
        "    final_labels = []\n",
        "    final_scores = []\n",
        "\n",
        "    for i, bbox in enumerate(filtered_boxes):\n",
        "        hog_features = extract_hog_features_from_bbox(img, bbox)\n",
        "        svm_pred = clf.predict([hog_features])\n",
        "        svm_prob = clf.predict_proba([hog_features])\n",
        "\n",
        "        if svm_pred[0] == 1:\n",
        "            final_boxes.append(bbox.tolist())\n",
        "            final_labels.append(filtered_labels[i].tolist())\n",
        "            final_scores.append(svm_prob[0][1])\n",
        "\n",
        "    return final_boxes, final_labels, final_scores\n"
      ],
      "metadata": {
        "id": "M_07RmPVO4Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We will show the images after svm is applied.\n",
        "def show_images_SVM_HOG(test_dataset, model, num_images, columns ):\n",
        "    rows = int(np.ceil(num_images / columns))\n",
        "    fig, axes = plt.subplots(rows, columns, figsize=(3 * columns, 3 * rows))\n",
        "   \n",
        "    axes = axes.ravel()\n",
        "    for ax in axes[num_images:]:\n",
        "        fig.delaxes(ax)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i in range(num_images):\n",
        "        img, _ = test_dataset[i]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = model([img.to(device)])[0]\n",
        "\n",
        "        nms_prediction = apply_nms(prediction, iou_thresh=0.01, score_thresh=0.5)\n",
        "\n",
        "        \n",
        "        img_np = img.cpu().numpy().transpose((1, 2, 0)) * 255\n",
        "        boxes, labels, scores = apply_svm_on_proposals(model, img_np, threshold=0.5)\n",
        "\n",
        "        svm_filtered_prediction = {\n",
        "            'boxes': torch.tensor(boxes),\n",
        "            'labels': torch.tensor(labels, dtype=torch.int64),\n",
        "            'scores': torch.tensor(scores)\n",
        "        }\n",
        "\n",
        "        plot_img_bbox(axes[i], torch_to_pil(img), svm_filtered_prediction)\n",
        " \n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "e_aio2thBKug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_images_SVM_HOG(dataset_test, model, num_images=4, columns=4)"
      ],
      "metadata": {
        "id": "Xt8bMlHWBM2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLQf0zV9btFm"
      },
      "source": [
        "#4. Yolov5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/yolov5 "
      ],
      "metadata": {
        "id": "4GF0OfEQeHB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THdwi1QpLtXG"
      },
      "outputs": [],
      "source": [
        "#We are importing the data from roboflow to implementation for yolov5.\n",
        "\n",
        "rf = Roboflow(api_key=\"7QnIUGHlluFrILkFBubf\")\n",
        "project = rf.workspace(\"adasd-ukmfp\").project(\"people_with_smartphone\")\n",
        "dataset = project.version(3).download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.1 Training"
      ],
      "metadata": {
        "id": "d2r-bNETzKSj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhMIoG99YQ_x"
      },
      "outputs": [],
      "source": [
        "# We are training yolov5 model here, we have trained the model with 150 epochs and used this fully trained model\n",
        "# at the end of the notebook.\n",
        "\n",
        "!python train.py --img 320 --epochs 1 --data people_with_smartphone-3/data.yaml --weights yolov5x.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.2 Testing"
      ],
      "metadata": {
        "id": "2hn-odR7zXus"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6ffHTKNa7jE"
      },
      "outputs": [],
      "source": [
        "#We are testing our model on the test dataset\n",
        "!python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.25 --data people_with_smartphone-3/data.yaml --source people_with_smartphone-3/test/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVgj6oEgc_0x"
      },
      "outputs": [],
      "source": [
        "#We are displaying the test images that the model predict\n",
        "\n",
        "image_directory = '/content/yolov5/runs/detect/exp'\n",
        "all_files = os.listdir(image_directory)\n",
        "\n",
        "image_filenames = [file for file in all_files if file.endswith(('.jpg', '.png'))]\n",
        "\n",
        "num_images = 4\n",
        "for i in range(num_images):\n",
        "    image_filename = image_filenames[i]\n",
        "    img = mpimg.imread(f\"{image_directory}/{image_filename}\")\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1xjvxFmsq1B"
      },
      "source": [
        "#5. Yolov8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku3i8a2sw83z"
      },
      "outputs": [],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfoA1O2tu-7n"
      },
      "outputs": [],
      "source": [
        "#We are uploading the dataset in yolov8 format.\n",
        "\n",
        "#rf = Roboflow(api_key=\"7QnIUGHlluFrILkFBubf\")\n",
        "#project = rf.workspace(\"adasd-ukmfp\").project(\"people_with_smartphone\")\n",
        "dataset = project.version(3).download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.1 Training"
      ],
      "metadata": {
        "id": "vUSc7Y9Qz1h5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "249dhps6stTb"
      },
      "outputs": [],
      "source": [
        "# We are load a pretrained YOLO model which is recommended for training in the official documentation.\n",
        "model_yolov8 = YOLO('yolov8x.pt')\n",
        "\n",
        "#We are training the model for one epoch, as we mentioned earlier we have trained this model for 150 epochs at the end of\n",
        "#the notebook we use it.\n",
        "results = model_yolov8.train(data='/content/people_with_smartphone-3/data.yaml', epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.2 Testing"
      ],
      "metadata": {
        "id": "hxdpRuh8z3kH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j18CJxW5tA-I"
      },
      "outputs": [],
      "source": [
        "#We are testing our trained model on test dataset\n",
        "predictions=model_yolov8.predict('/content/people_with_smartphone-3/test/images', save=True, imgsz=640, conf=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOLeKJRtx2um"
      },
      "outputs": [],
      "source": [
        "#Displaying the images that trained model predict\n",
        "\n",
        "image_directory = 'runs/detect/predict'\n",
        "all_files = os.listdir(image_directory)\n",
        "\n",
        "image_filenames = [file for file in all_files if file.endswith(('.jpg', '.png'))]\n",
        "\n",
        "num_images = 10\n",
        "for i in range(num_images):\n",
        "    image_filename = image_filenames[i]\n",
        "    img = mpimg.imread(f\"{image_directory}/{image_filename}\")\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Importing Fully Trained Models"
      ],
      "metadata": {
        "id": "cBKlTRYJ2XH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have trained Faster CNN for 10 epochs, yolov5 and yolov8 for 150 epochs. Since these models are large, we upload them to dropbox and we will import them because training these models takes huge amount of time, faster cnn and svm takes about more than two hours for 10 epochs, yolov5 takes 2.3 hours for 150 epochs and yolov8 takes about 2.5 hours for 150 epochs."
      ],
      "metadata": {
        "id": "hILgAKBi2Xr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are uploading trained models from dropbox and unzipping these files\n",
        "!wget -O fully_trained_models.zip https://www.dropbox.com/s/f8uc0p3j1yqrznp/full_training.zip?dl=0\n",
        "!unzip fully_trained_models.zip"
      ],
      "metadata": {
        "id": "tAX-UVx_dThM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.1 Loading FasterCNN"
      ],
      "metadata": {
        "id": "DK7k6OI42LwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are loading fully trained model of fastercnn\n",
        "\n",
        "full_model_fasterCNN = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=2, pretrained_backbone=True) \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        " \n",
        "model_load_path = \"/content/faster_cnn_10_epochs.pth\"    \n",
        "full_model_fasterCNN.load_state_dict(torch.load(model_load_path))\n",
        "full_model_fasterCNN = full_model_fasterCNN.to(device)\n"
      ],
      "metadata": {
        "id": "oceaDzkGy8B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.2 Defining Display Functions\n"
      ],
      "metadata": {
        "id": "IFcsz5lB_VNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are defining the same functions that we use before but we are defining them to be producing random\n",
        "#images and also functions will be used upon on whole test dataset. So we need to define them again.\n",
        "#for avoiding repeating codes we might use functions defined earlier but to avoid CUDA Memory error,\n",
        "#we had to define them again\n",
        "\n",
        "def apply_svm_on_proposals(model, clf, img, threshold=0.5):\n",
        " \n",
        "    img_tensor = torch.tensor(img / 255, dtype=torch.float).permute(2, 0, 1).to(device)\n",
        "    img_tensor = img_tensor.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_tensor)\n",
        "\n",
        "    scores = outputs[0]['scores']\n",
        "    boxes = outputs[0]['boxes']\n",
        "    labels = outputs[0]['labels']\n",
        "\n",
        "    filtered_indices = [idx for idx, score in enumerate(scores) if score > threshold]\n",
        "    filtered_boxes = boxes[filtered_indices]\n",
        "    filtered_labels = labels[filtered_indices]\n",
        "\n",
        "    final_boxes = []\n",
        "    final_labels = []\n",
        "    final_scores = []\n",
        "\n",
        "    for i, bbox in enumerate(filtered_boxes):\n",
        "        hog_features = extract_hog_features_from_bbox(img, bbox)\n",
        "        svm_pred = clf.predict([hog_features])\n",
        "        svm_prob = clf.predict_proba([hog_features])\n",
        "\n",
        "        if svm_pred[0] == 1:\n",
        "            final_boxes.append(bbox.tolist())\n",
        "            final_labels.append(filtered_labels[i].tolist())\n",
        "            final_scores.append(svm_prob[0][1])\n",
        "\n",
        "    return final_boxes, final_labels, final_scores\n",
        "def show_random_images_SVM_HOG(test_dataset, model, svm_model,num_images, columns, random_seed=None):\n",
        "    rows = int(np.ceil(num_images / columns))\n",
        "    fig, axes = plt.subplots(rows, columns, figsize=(3 * columns, 3 * rows))\n",
        "\n",
        "    axes = axes.ravel()\n",
        "    for ax in axes[num_images:]:\n",
        "        fig.delaxes(ax)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if random_seed is not None:\n",
        "        random.seed(random_seed)\n",
        "\n",
        "    selected_indices = random.sample(range(len(test_dataset)), num_images)\n",
        "\n",
        "    for i, idx in enumerate(selected_indices):\n",
        "        img, _ = test_dataset[idx]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = model([img.to(device)])[0]\n",
        "\n",
        "        nms_prediction = apply_nms(prediction, iou_thresh=0.01, score_thresh=0.9)\n",
        "\n",
        "        \n",
        "        img_np = img.cpu().numpy().transpose((1, 2, 0)) * 255\n",
        "        boxes, labels, scores = apply_svm_on_proposals(model,svm_model, img_np, threshold=0.9)\n",
        "\n",
        "        svm_filtered_prediction = {\n",
        "            'boxes': torch.tensor(boxes),\n",
        "            'labels': torch.tensor(labels, dtype=torch.int64),\n",
        "            'scores': torch.tensor(scores)\n",
        "        }\n",
        "\n",
        "        plot_img_bbox(axes[i], torch_to_pil(img), svm_filtered_prediction)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "def show_random_images_faster_cnn(test_dataset, model, num_images, columns=2, random_seed=None):\n",
        "    rows = int(np.ceil(num_images / columns))\n",
        "    fig, axes = plt.subplots(rows, columns, figsize=(3 * columns, 3 * rows))\n",
        "\n",
        "    \n",
        "    axes = axes.ravel()\n",
        "    for ax in axes[num_images:]:\n",
        "        fig.delaxes(ax)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if random_seed is not None:\n",
        "        random.seed(random_seed)\n",
        "\n",
        "    selected_indices = random.sample(range(len(test_dataset)), num_images)\n",
        "\n",
        "    for i, idx in enumerate(selected_indices):\n",
        "        img, _ = test_dataset[idx]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = model([img.to(device)])[0]\n",
        "\n",
        "        nms_prediction = apply_nms(prediction, iou_thresh=0.01, score_thresh=0.01)\n",
        "        plot_img_bbox(axes[i], torch_to_pil(img), nms_prediction)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1uD9srA5dH8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.3 Loading SVM"
      ],
      "metadata": {
        "id": "LUeKBoDl2jxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are loading svm classifier that we trained fully.\n",
        "svm_model = load('/content/svm_classifier.joblib')"
      ],
      "metadata": {
        "id": "7pbVzxa82oGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.4 Comparing CNN and Hybrid Approach"
      ],
      "metadata": {
        "id": "Tq-zuM7N16v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Results before SVM and HOG\n",
        "random_seed=42\n",
        "num_images=10\n",
        "columns=5\n",
        "show_random_images_faster_cnn(dataset_test, full_model_fasterCNN, num_images=num_images, columns=columns,random_seed=random_seed)"
      ],
      "metadata": {
        "id": "RQcH_2FS8iSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Results after SVM and HOG\n",
        "show_random_images_SVM_HOG(dataset_test, full_model_fasterCNN,svm_model, num_images=num_images, columns=columns,random_seed=random_seed)"
      ],
      "metadata": {
        "id": "opfEFPHOdtha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.5 Loading Yolov5"
      ],
      "metadata": {
        "id": "df26vIsz2DCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are loading yolov5 model that we trained fully."
      ],
      "metadata": {
        "id": "cE9wMQBwzB4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov5"
      ],
      "metadata": {
        "id": "tz6vz2XjcLO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the changes of metrics during training.\n",
        "image = Image.open('/content/yolov5_150_results.png')\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9aHXaU98hlFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are making predictions on test dataset by using our fully trained yolov5 model.\n",
        "!python detect.py --weights /content/yolov5_150_weights.pt --img 640 --conf 0.25 --data /content/people_with_smartphone-3/data.yaml --source /content/people_with_smartphone-3/test/images"
      ],
      "metadata": {
        "id": "wpvXwxSWcn28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying images that model predicted.\n",
        "\n",
        "image_directory = 'runs/detect/exp2'\n",
        "all_files = os.listdir(image_directory)\n",
        "\n",
        "image_filenames = [file for file in all_files if file.endswith(('.jpg', '.png'))]\n",
        "\n",
        "num_images = 5\n",
        "for i in range(num_images):\n",
        "    image_filename = image_filenames[i]\n",
        "    img = mpimg.imread(f\"{image_directory}/{image_filename}\")\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "3vA_x9ii63vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.7 Loading Yolov8"
      ],
      "metadata": {
        "id": "y52lX-Bm2OXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are loading yolov8 model that we trained fully.\n",
        "model_full_yolov8 = YOLO('/content/yolov8_150_weights.pt')"
      ],
      "metadata": {
        "id": "NbhXS81wwge5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the changes of metrics during training.\n",
        "image = Image.open('/content/yolov8_150_results.png')\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AeS7bkH7g1FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hpnrrs4PKpw"
      },
      "source": [
        "##6.7 Testing Yolov8 Model On Video"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We are testing the fully trained yolov8 model on the video. \n",
        "\n",
        "video_file = '/content/text_walkers.mp4'\n",
        "predictions=model_full_yolov8.predict(video_file, save=True, imgsz=640, conf=0.8)"
      ],
      "metadata": {
        "id": "qoWhPlRBDAkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIyk-lTiNqWL"
      },
      "outputs": [],
      "source": [
        "video_file_predicted='/content/yolov5/runs/detect/predict/text_walkers.mp4'\n",
        "clip = VideoFileClip(video_file_predicted)\n",
        "clip_resized = clip.resize(height=360)  \n",
        "temp_video_file = \"/content/temp.mp4\"\n",
        "clip_resized.write_videofile(temp_video_file, codec=\"libx264\")\n",
        "\n",
        "with open(temp_video_file, \"rb\") as f:\n",
        "    video_data = b64encode(f.read()).decode()\n",
        "\n",
        "HTML(f\"\"\"\n",
        "<video width=\"80%\" controls>\n",
        "  <source src=\"data:video/mp4;base64,{video_data}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")\n"
      ]
    }
  ]
}